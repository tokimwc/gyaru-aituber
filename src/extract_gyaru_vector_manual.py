"""
Gyaru Vector Extraction Script (Manual Implementation)

repengã‚’ä½¿ã‚ãšã€PyTorch Hookã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹ã€‚
Qwen2.5ã¨ã®äº’æ›æ€§å•é¡Œã‚’å›é¿ã€‚
"""

import json
import logging
import torch
import numpy as np
from pathlib import Path
from typing import List, Optional, Dict
import argparse
import yaml
from sklearn.decomposition import PCA

from transformers import AutoModelForCausalLM, AutoTokenizer

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path: Optional[Path] = None):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if config_path is None:
        config_path = Path("config/generation_config.yaml")
    
    if not config_path.exists():
        logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return None
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def find_latest_dataset(dataset_dir: Path = Path("outputs/processed")) -> Path:
    """æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    if not dataset_dir.exists():
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_dir}")
    
    dataset_files = list(dataset_dir.glob("gyaru_dataset_*.json"))
    if not dataset_files:
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_dir}")
    
    latest_file = max(dataset_files, key=lambda p: p.stat().st_mtime)
    logger.info(f"æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")
    return latest_file


def load_dataset(dataset_path: Path) -> List[dict]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­: {dataset_path}")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(data)}ãƒšã‚¢")
    return data


def extract_hidden_states_manual(
    model,
    tokenizer,
    dataset: List[dict],
    target_layers: List[int],
    max_entries: Optional[int] = None
) -> Dict[int, np.ndarray]:
    """æ‰‹å‹•ã§Hidden Statesã‚’æŠ½å‡ºã—ã¦PCAã‚’å®Ÿè¡Œ"""
    
    if max_entries:
        dataset = dataset[:max_entries]
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ {max_entries} ä»¶ã«åˆ¶é™ã—ã¾ã—ãŸ")
    
    # å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´
    diffs = {layer: [] for layer in target_layers}
    
    logger.info(f"Hidden StatesæŠ½å‡ºã‚’é–‹å§‹ï¼ˆ{len(dataset)}ãƒšã‚¢ï¼‰...")
    
    def get_hook(storage_list):
        """Hooké–¢æ•°: ç‰¹å®šã®å±¤ã®å‡ºåŠ›ã‚’å–å¾—"""
        def hook(module, input, output):
            # outputã¯é€šå¸¸ (hidden_states, ...) ã®ã‚¿ãƒ—ãƒ«
            if isinstance(output, tuple):
                hidden = output[0]
            else:
                hidden = output
            # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®Hidden Stateã‚’å–å¾—
            # shape: [batch, seq_len, hidden_dim] -> [batch, hidden_dim]
            storage_list.append(hidden[:, -1, :].detach().cpu())
        return hook
    
    # å„ãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ã«å¯¾ã—ã¦æ¨è«–
    for idx, item in enumerate(dataset):
        if (idx + 1) % 10 == 0:
            logger.info(f"  {idx + 1}/{len(dataset)} ãƒšã‚¢ã‚’å‡¦ç†ä¸­...")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        user_msg = f"ãƒˆãƒ”ãƒƒã‚¯: {item.get('topic', 'é›‘è«‡')}\nã“ã‚Œã«ã¤ã„ã¦è©±ã—ã¦ã€‚"
        
        # Standardç”¨ã®å…¥åŠ›
        msgs_std = [
            {"role": "user", "content": user_msg},
            {"role": "assistant", "content": item["standard"]}
        ]
        input_std = tokenizer.apply_chat_template(
            msgs_std,
            tokenize=True,
            return_tensors="pt",
            add_generation_prompt=False
        ).to(model.device)
        
        # Gyaruç”¨ã®å…¥åŠ›
        msgs_gya = [
            {"role": "user", "content": user_msg},
            {"role": "assistant", "content": item["gyaru"]}
        ]
        input_gya = tokenizer.apply_chat_template(
            msgs_gya,
            tokenize=True,
            return_tensors="pt",
            add_generation_prompt=False
        ).to(model.device)
        
        # Standard Pass
        std_activations = {}
        handles = []
        for layer_idx in target_layers:
            std_activations[layer_idx] = []
            layer_module = model.model.layers[layer_idx]
            handle = layer_module.register_forward_hook(
                get_hook(std_activations[layer_idx])
            )
            handles.append(handle)
        
        with torch.no_grad():
            model(input_std)
        
        # Hookè§£é™¤
        for h in handles:
            h.remove()
        
        # Gyaru Pass
        gya_activations = {}
        handles = []
        for layer_idx in target_layers:
            gya_activations[layer_idx] = []
            layer_module = model.model.layers[layer_idx]
            handle = layer_module.register_forward_hook(
                get_hook(gya_activations[layer_idx])
            )
            handles.append(handle)
        
        with torch.no_grad():
            model(input_gya)
        
        # Hookè§£é™¤
        for h in handles:
            h.remove()
        
        # å·®åˆ†ã‚’è¨ˆç®—ã—ã¦è“„ç©
        for layer_idx in target_layers:
            if std_activations[layer_idx] and gya_activations[layer_idx]:
                vec_std = std_activations[layer_idx][0]  # [1, hidden_dim]
                vec_gya = gya_activations[layer_idx][0]
                
                # å·®åˆ† = ã‚®ãƒ£ãƒ« - æ¨™æº–
                diff = vec_gya - vec_std
                diffs[layer_idx].append(diff)
    
    logger.info("PCAã§ä¸»æˆåˆ†ã‚’è¨ˆç®—ä¸­...")
    final_vectors = {}
    
    for layer_idx, diff_list in diffs.items():
        if not diff_list:
            logger.warning(f"ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_idx} ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        
        # [num_samples, hidden_dim]
        X = torch.cat(diff_list, dim=0).float().numpy()
        
        # PCAã§ç¬¬1ä¸»æˆåˆ†ã‚’æŠ½å‡º
        pca = PCA(n_components=1)
        pca.fit(X)
        
        # æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«
        direction = pca.components_[0]  # [hidden_dim]
        
        # ç¬¦å·ã®èª¿æ•´: ã‚®ãƒ£ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã¨ã®å†…ç©ãŒæ­£ã«ãªã‚‹ã‚ˆã†ã«ã™ã‚‹
        mean_diff = np.mean(X, axis=0)
        if np.dot(direction, mean_diff) < 0:
            direction = -direction
        
        final_vectors[layer_idx] = torch.tensor(direction, dtype=torch.float16)
        
        logger.info(f"  ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_idx}: åˆ†æ•£èª¬æ˜ç‡ {pca.explained_variance_ratio_[0]:.4f}")
    
    return final_vectors


def save_vectors(vectors: Dict[int, torch.Tensor], output_dir: Path, model_name: str):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # PyTorchå½¢å¼ã§ä¿å­˜
    pt_path = output_dir / f"{model_name}_gyaru_vector_manual.pt"
    logger.info(f"PyTorchå½¢å¼ã§ä¿å­˜ä¸­: {pt_path}")
    torch.save(vectors, pt_path)
    logger.info(f"âœ… ä¿å­˜å®Œäº†: {pt_path}")
    
    # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
    stats_path = output_dir / f"{model_name}_gyaru_vector_stats.txt"
    with open(stats_path, 'w', encoding='utf-8') as f:
        f.write("# Gyaru Vector Statistics\n\n")
        f.write(f"Total layers: {len(vectors)}\n")
        f.write(f"Layer indices: {sorted(vectors.keys())}\n\n")
        for layer_idx in sorted(vectors.keys()):
            vec = vectors[layer_idx]
            f.write(f"Layer {layer_idx}:\n")
            f.write(f"  Shape: {vec.shape}\n")
            f.write(f"  Norm: {vec.norm().item():.4f}\n")
            f.write(f"  Mean: {vec.mean().item():.6f}\n")
            f.write(f"  Std: {vec.std().item():.6f}\n\n")
    
    logger.info(f"âœ… çµ±è¨ˆæƒ…å ±ä¿å­˜å®Œäº†: {stats_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Gyaru Vector Extraction (Manual)")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/generation_config.yaml"),
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--dataset",
        type=Path,
        default=None,
        help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯æœ€æ–°ã‚’ä½¿ç”¨ï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="ãƒ¢ãƒ‡ãƒ«åï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€å ´åˆã¯çœç•¥ï¼‰"
    )
    parser.add_argument(
        "--max-entries",
        type=int,
        default=50,
        help="ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ï¼‰"
    )
    parser.add_argument(
        "--layers",
        type=str,
        default="15-40",
        help="å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ç¯„å›²ï¼ˆä¾‹: 15-40ã€14Bãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("outputs/vectors"),
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    args = parser.parse_args()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config(args.config)
    
    # ãƒ¢ãƒ‡ãƒ«åã®æ±ºå®š
    if args.model:
        model_name = args.model
    elif config and 'model' in config and 'path' in config['model']:
        model_name = config['model']['path']
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®models/ã‹ã‚‰æ¢ã™
        model_name = "models/Qwen2.5-14B-Instruct-bnb-4bit"
    
    logger.info(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    if args.dataset:
        dataset_path = args.dataset
    else:
        dataset_path = find_latest_dataset()
    
    raw_data = load_dataset(dataset_path)
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ç¯„å›²ã®è§£æ
    layer_start, layer_end = map(int, args.layers.split('-'))
    target_layers = list(range(layer_start, layer_end + 1))
    logger.info(f"å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼: {target_layers[0]}-{target_layers[-1]} ({len(target_layers)}ãƒ¬ã‚¤ãƒ¤ãƒ¼)")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    logger.info("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    logger.info("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype="auto"  # bitsandbytes 4bité‡å­åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚"auto"ã«å¤‰æ›´
    )
    model.eval()
    
    # ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º
    vectors = extract_hidden_states_manual(
        model=model,
        tokenizer=tokenizer,
        dataset=raw_data,
        target_layers=target_layers,
        max_entries=args.max_entries
    )
    
    # ä¿å­˜
    model_name_short = model_name.split('/')[-1]
    save_vectors(vectors, args.output_dir, model_name_short)
    
    logger.info("ğŸ‰ ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºå®Œäº†ï¼")
    logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")


if __name__ == "__main__":
    main()
