"""
Gyaru Vector Extraction Script

RepE (Representation Engineering) ã‚’ä½¿ç”¨ã—ã¦ã€
Qwen2.5-32B-Instruct-bnb-4bit ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã€Œã‚®ãƒ£ãƒ«æˆåˆ†ã€ã‚’è¡¨ã™åˆ¶å¾¡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹ã€‚
"""

import json
import logging
import torch
from pathlib import Path
from typing import List, Optional
import argparse
import yaml

from transformers import AutoModelForCausalLM, AutoTokenizer
from repeng import ControlVector, ControlModel, DatasetEntry

# --- ğŸ”¥ ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒé–‹å§‹ ğŸ”¥ ---
# repengã®ControlModuleãŒå…ƒã®ãƒ¢ãƒ‡ãƒ«å±æ€§ï¼ˆattention_typeç­‰ï¼‰ã‚’
# é€šéã•ã›ã‚‹ã‚ˆã†ã«ç„¡ç†ã‚„ã‚Šæ©Ÿèƒ½ã‚’æ‹¡å¼µã—ã¾ã™ã€‚
from repeng.control import ControlModule

def getattr_monkey_patch(self, name):
    """ControlModuleè‡ªèº«ãŒæŒã£ã¦ã„ãªã„å±æ€§ã¯ã€ãƒ©ãƒƒãƒ—ã—ã¦ã„ã‚‹moduleã‹ã‚‰æ¢ã™"""
    try:
        # ã¾ãšè‡ªåˆ†è‡ªèº«ã®å±æ€§ã‚’ç¢ºèª
        return object.__getattribute__(self, name)
    except AttributeError:
        # ãªã‘ã‚Œã°ãƒ©ãƒƒãƒ—ã—ã¦ã„ã‚‹moduleã‹ã‚‰å–å¾—
        module = object.__getattribute__(self, "module")
        return getattr(module, name)

# ã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ³¨å…¥
ControlModule.__getattr__ = getattr_monkey_patch
# --- ğŸ”¥ ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ‘ãƒƒãƒçµ‚äº† ğŸ”¥ ---

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path: Optional[Path] = None):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if config_path is None:
        config_path = Path("config/generation_config.yaml")
    
    if not config_path.exists():
        logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return None
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def find_latest_dataset(dataset_dir: Path = Path("outputs/processed")) -> Path:
    """æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    if not dataset_dir.exists():
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_dir}")
    
    dataset_files = list(dataset_dir.glob("gyaru_dataset_*.json"))
    if not dataset_files:
        raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_dir}")
    
    # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    latest_file = max(dataset_files, key=lambda p: p.stat().st_mtime)
    logger.info(f"æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {latest_file}")
    return latest_file


def load_dataset(dataset_path: Path) -> List[dict]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­: {dataset_path}")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(data)}ãƒšã‚¢")
    return data


def create_dataset_entries(
    raw_data: List[dict],
    tokenizer: AutoTokenizer,
    max_entries: Optional[int] = None
) -> List[DatasetEntry]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’DatasetEntryå½¢å¼ã«å¤‰æ›"""
    logger.info("DatasetEntryã‚’ä½œæˆä¸­...")
    
    # ãƒ‡ãƒ¼ã‚¿æ•°ã‚’åˆ¶é™ï¼ˆOOMå¯¾ç­–ï¼‰
    if max_entries:
        raw_data = raw_data[:max_entries]
        logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•°ã‚’ {max_entries} ä»¶ã«åˆ¶é™ã—ã¾ã—ãŸ")
    
    dataset = []
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
    user_prompts = [
        "ãã®å†…å®¹ã«ã¤ã„ã¦è©±ã—ã¦ã€‚",
        "ã©ã†æ€ã„ã¾ã™ã‹ï¼Ÿ",
        "è©³ã—ãæ•™ãˆã¦ã€‚",
        "åå¿œã—ã¦ãã ã•ã„ã€‚",
        "æ„Ÿæƒ³ã‚’æ•™ãˆã¦ã€‚"
    ]
    
    import random
    
    for i, item in enumerate(raw_data):
        # standardã¨gyaruã®æ–‡ç« ã‚’å–å¾—
        positive = item["gyaru"]  # ã‚®ãƒ£ãƒ«å£èª¿
        negative = item["standard"]  # æ¨™æº–èª
        
        # ãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ã„ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        topic = item.get('topic', 'é›‘è«‡')
        user_text = f"ãƒˆãƒ”ãƒƒã‚¯: {topic}\nã“ã‚Œã«ã¤ã„ã¦è©±ã—ã¦ã€‚"
        
        # Chat Templateã®é©ç”¨
        messages = [{"role": "user", "content": user_text}]
        prefix = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        dataset.append(DatasetEntry(
            positive=f"{prefix}{positive}",
            negative=f"{prefix}{negative}"
        ))
        
        if (i + 1) % 20 == 0:
            logger.info(f"  {i + 1}/{len(raw_data)} ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    logger.info(f"DatasetEntryä½œæˆå®Œäº†: {len(dataset)}ã‚¨ãƒ³ãƒˆãƒª")
    return dataset


def extract_vector(
    model_name: str,
    dataset: List[DatasetEntry],
    target_layers: List[int],
    batch_size: int = 2,
    device_map: str = "auto",
    trust_remote_code: bool = True
) -> ControlVector:
    """åˆ¶å¾¡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º"""
    logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {model_name}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=trust_remote_code
    )
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆæ—¢ã«4bité‡å­åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾ãƒ­ãƒ¼ãƒ‰ï¼‰
    logger.info("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ï¼ˆæ—¢ã«4bité‡å­åŒ–æ¸ˆã¿ï¼‰...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map=device_map,
        trust_remote_code=trust_remote_code
        # ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«4bité‡å­åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€load_in_4bitã¯ä¸è¦
    )
    
    # RepEç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒƒãƒ—ï¼ˆå¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ï¼‰
    logger.info(f"ControlModelã«ãƒ©ãƒƒãƒ—ä¸­ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼: {target_layers[0]}-{target_layers[-1]}ï¼‰...")
    model = ControlModel(model, target_layers)
    
    # ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º
    logger.info(f"ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºã‚’é–‹å§‹ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}ï¼‰...")
    vector = ControlVector.train(
        model,
        tokenizer,
        dataset,
        batch_size=batch_size
    )
    
    logger.info("ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºå®Œäº†")
    return vector


def save_vector(vector: ControlVector, output_dir: Path, model_name: str):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜"""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # GGUFå½¢å¼ã§ä¿å­˜
    gguf_path = output_dir / f"{model_name.replace('/', '_').replace('-', '_')}_gyaru_vector.gguf"
    logger.info(f"GGUFå½¢å¼ã§ä¿å­˜ä¸­: {gguf_path}")
    try:
        vector.export_gguf(str(gguf_path))
        logger.info(f"âœ… GGUFå½¢å¼ã§ä¿å­˜å®Œäº†: {gguf_path}")
    except Exception as e:
        logger.warning(f"âš ï¸  GGUFå½¢å¼ã§ã®ä¿å­˜ã«å¤±æ•—: {e}")
    
    # PyTorchå½¢å¼ã§ä¿å­˜
    pt_path = output_dir / "gyaru_vector_obj.pt"
    logger.info(f"PyTorchå½¢å¼ã§ä¿å­˜ä¸­: {pt_path}")
    try:
        torch.save(vector, pt_path)
        logger.info(f"âœ… PyTorchå½¢å¼ã§ä¿å­˜å®Œäº†: {pt_path}")
    except Exception as e:
        logger.warning(f"âš ï¸  PyTorchå½¢å¼ã§ã®ä¿å­˜ã«å¤±æ•—: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="Gyaru Vector Extraction")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/generation_config.yaml"),
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--dataset",
        type=Path,
        default=None,
        help="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯æœ€æ–°ã‚’ä½¿ç”¨ï¼‰"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="ãƒ¢ãƒ‡ãƒ«åï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€å ´åˆã¯çœç•¥ï¼‰"
    )
    parser.add_argument(
        "--max-entries",
        type=int,
        default=None,
        help="ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°ï¼ˆOOMå¯¾ç­–ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=2,
        help="ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰"
    )
    parser.add_argument(
        "--layers",
        type=str,
        default="20-50",
        help="å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ç¯„å›²ï¼ˆä¾‹: 20-50ï¼‰"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("outputs/vectors"),
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    args = parser.parse_args()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config(args.config)
    
    # ãƒ¢ãƒ‡ãƒ«åã®æ±ºå®š
    if args.model:
        model_name = args.model
    elif config and 'model' in config and 'path' in config['model']:
        model_name = config['model']['path']
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®models/ã‹ã‚‰æ¢ã™
        model_name = "models/Qwen2.5-32B-Instruct-bnb-4bit"
    
    logger.info(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_name}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    if args.dataset:
        dataset_path = args.dataset
    else:
        dataset_path = find_latest_dataset()
    
    raw_data = load_dataset(dataset_path)
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ï¼ˆDatasetEntryä½œæˆç”¨ï¼‰
    logger.info("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    
    # DatasetEntryä½œæˆ
    dataset_entries = create_dataset_entries(
        raw_data,
        tokenizer,
        max_entries=args.max_entries
    )
    
    # ãƒ¬ã‚¤ãƒ¤ãƒ¼ç¯„å›²ã®è§£æ
    layer_start, layer_end = map(int, args.layers.split('-'))
    target_layers = list(range(layer_start, layer_end + 1))
    logger.info(f"å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼: {target_layers[0]}-{target_layers[-1]} ({len(target_layers)}ãƒ¬ã‚¤ãƒ¤ãƒ¼)")
    
    # ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡º
    vector = extract_vector(
        model_name=model_name,
        dataset=dataset_entries,
        target_layers=target_layers,
        batch_size=args.batch_size,
        trust_remote_code=True
    )
    
    # ä¿å­˜
    model_name_short = model_name.split('/')[-1]
    save_vector(vector, args.output_dir, model_name_short)
    
    logger.info("ğŸ‰ ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºå®Œäº†ï¼")
    logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.output_dir}")


if __name__ == "__main__":
    main()
