"""
Gyaru Dataset Generator - Main Script (vLLM APIç‰ˆ)

vLLM OpenAIäº’æ›APIã‚’ä½¿ç”¨ã—ã¦ã€Œã‚ãƒ¼ã—ç³»ã‚®ãƒ£ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
"""

import json
import logging
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
import yaml
from openai import OpenAI
from rich.console import Console
from rich.progress import track
from pydantic import BaseModel

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
console = Console()


class DatasetConfig(BaseModel):
    """è¨­å®šã‚¯ãƒ©ã‚¹"""
    config_path: Path = Path("config/generation_config.yaml")
    api_base: str = "http://localhost:8000/v1"
    num_pairs_per_batch: int = 30
    total_batches: int = 4
    temperature: float = 0.8
    top_p: float = 0.9
    max_new_tokens: int = 2000
    output_dir: Path = Path("outputs")
    require_ash: bool = True


class GyaruDatasetGenerator:
    """ã‚®ãƒ£ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå™¨ï¼ˆvLLM APIç‰ˆï¼‰"""
    
    def __init__(self, config: DatasetConfig):
        self.config = config
        self.console = Console()
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        self.model_name = None
        if config.config_path.exists():
            with open(config.config_path, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’èª­ã¿è¾¼ã¿
                if 'model' in yaml_config and 'path' in yaml_config['model']:
                    self.model_name = yaml_config['model']['path']
                # YAMLè¨­å®šã‹ã‚‰å€¤ã‚’ä¸Šæ›¸ã
                if 'vllm' in yaml_config:
                    self.config.api_base = yaml_config['vllm'].get('api_base', self.config.api_base)
                if 'generation' in yaml_config:
                    gen_config = yaml_config['generation']
                    self.config.num_pairs_per_batch = gen_config.get('num_pairs_per_batch', self.config.num_pairs_per_batch)
                    self.config.total_batches = gen_config.get('total_batches', self.config.total_batches)
                    self.config.temperature = gen_config.get('temperature', self.config.temperature)
                    self.config.top_p = gen_config.get('top_p', self.config.top_p)
                    self.config.max_new_tokens = gen_config.get('max_new_tokens', self.config.max_new_tokens)
                if 'output' in yaml_config:
                    self.config.output_dir = Path(yaml_config['output'].get('output_dir', self.config.output_dir))
                if 'validation' in yaml_config:
                    self.config.require_ash = yaml_config['validation'].get('require_ash', self.config.require_ash)
        
        # ãƒ¢ãƒ‡ãƒ«åãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        if not self.model_name:
            self.model_name = "models/Qwen2.5-32B-Instruct-bnb-4bit"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿
        prompt_path = Path("prompts/system_prompt.txt")
        if not prompt_path.exists():
            raise FileNotFoundError(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prompt_path}")
        
        with open(prompt_path, 'r', encoding='utf-8') as f:
            self.system_prompt = f.read()
        
        # OpenAIäº’æ›APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        logger.info(f"ğŸ”Œ Connecting to vLLM API: {self.config.api_base}")
        self.client = OpenAI(
            base_url=self.config.api_base,
            api_key="not-needed"  # vLLMã¯APIã‚­ãƒ¼ä¸è¦
        )
        
        # ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª
        self._check_server_connection()
    
    def _check_server_connection(self, max_retries: int = 5, retry_delay: int = 2):
        """vLLMã‚µãƒ¼ãƒãƒ¼ã¸ã®æ¥ç¶šã‚’ç¢ºèª"""
        for i in range(max_retries):
            try:
                # ç°¡å˜ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æ¥ç¶šç¢ºèª
                models = self.client.models.list()
                logger.info("âœ… vLLMã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šæˆåŠŸ")
                return True
            except Exception as e:
                if i < max_retries - 1:
                    logger.warning(f"âš ï¸  ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šå¤±æ•— (è©¦è¡Œ {i+1}/{max_retries}): {e}")
                    logger.info(f"â³ {retry_delay}ç§’å¾Œã«å†è©¦è¡Œ...")
                    time.sleep(retry_delay)
                else:
                    raise ConnectionError(
                        f"âŒ vLLMã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“: {self.config.api_base}\n"
                        f"   ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„: python src/start_vllm_server.py"
                    ) from e
        return False
    
    def generate_batch(self, batch_num: int) -> List[Dict]:
        """1ãƒãƒƒãƒåˆ†ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        logger.info(f"ãƒãƒƒãƒ {batch_num}/{self.config.total_batches} ã‚’ç”Ÿæˆä¸­...")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        messages = [
            {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªã‚·ãƒŠãƒªã‚ªãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚JSONå½¢å¼ã§ã®ã¿å¿œç­”ã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": self.system_prompt}
        ]
        
        # vLLM APIã§ç”Ÿæˆ
        try:
            # ãƒ¢ãƒ‡ãƒ«åã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã å€¤ã‚’ä½¿ç”¨
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                max_tokens=self.config.max_new_tokens
            )
            
            generated_text = response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"âŒ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {batch_num}): {e}")
            return []
        
        # JSONæŠ½å‡º
        try:
            # ```json ... ``` ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’å‡¦ç†
            if "```json" in generated_text:
                json_start = generated_text.find("```json") + 7
                json_end = generated_text.find("```", json_start)
                json_str = generated_text[json_start:json_end].strip()
            elif "```" in generated_text:
                # ``` ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆè¨€èªæŒ‡å®šãªã—ï¼‰
                json_start = generated_text.find("```") + 3
                json_end = generated_text.find("```", json_start)
                json_str = generated_text[json_start:json_end].strip()
            elif "[" in generated_text:
                # [ ã‹ã‚‰å§‹ã¾ã‚‹éƒ¨åˆ†ã‚’æŠ½å‡º
                json_start = generated_text.find("[")
                json_end = generated_text.rfind("]") + 1
                json_str = generated_text[json_start:json_end]
            else:
                json_str = generated_text.strip()
            
            data = json.loads(json_str)
            
            # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
            if not isinstance(data, list):
                data = [data]
            
            # IDã‚’è¿½åŠ 
            for i, item in enumerate(data):
                item["id"] = (batch_num - 1) * self.config.num_pairs_per_batch + i + 1
                item["batch"] = batch_num
                item["generated_at"] = datetime.now().isoformat()
            
            logger.info(f"âœ… ãƒãƒƒãƒ {batch_num}: {len(data)}ãƒšã‚¢ç”ŸæˆæˆåŠŸ")
            return data
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {batch_num}): {e}")
            logger.error(f"ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:\n{generated_text[:500]}...")
            return []
    
    def validate_dataset(self, data: List[Dict]) -> List[Dict]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªæ¤œè¨¼"""
        validated = []
        for item in data:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
            if not all(k in item for k in ["topic", "standard", "gyaru"]):
                logger.warning(f"âŒ ç„¡åŠ¹ãªãƒ‡ãƒ¼ã‚¿ (ID: {item.get('id', 'N/A')}): å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸è¶³")
                continue
            
            # ã€Œã‚ãƒ¼ã—ã€ãƒã‚§ãƒƒã‚¯
            if self.config.require_ash and "ã‚ãƒ¼ã—" not in item["gyaru"]:
                logger.warning(f"âš ï¸  ã€Œã‚ãƒ¼ã—ã€ãªã— (ID: {item['id']}): {item['gyaru'][:50]}...")
                # è­¦å‘Šã ã‘å‡ºã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã¯æ®‹ã™
            
            validated.append(item)
        
        logger.info(f"âœ… æ¤œè¨¼å®Œäº†: {len(validated)}/{len(data)}ãƒšã‚¢ãŒæœ‰åŠ¹")
        return validated
    
    def generate_dataset(self) -> Path:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’ç”Ÿæˆ"""
        all_data = []
        
        self.console.print(f"[bold cyan]ğŸš€ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆé–‹å§‹[/bold cyan]")
        self.console.print(f"ç›®æ¨™: {self.config.total_batches}ãƒãƒƒãƒ Ã— {self.config.num_pairs_per_batch}ãƒšã‚¢ = {self.config.total_batches * self.config.num_pairs_per_batch}ãƒšã‚¢\n")
        
        for batch_num in track(range(1, self.config.total_batches + 1), description="ç”Ÿæˆä¸­..."):
            batch_data = self.generate_batch(batch_num)
            if batch_data:
                all_data.extend(batch_data)
                
                # ãƒãƒƒãƒã”ã¨ã«ä¿å­˜ (ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)
                batch_file = self.config.output_dir / "raw" / f"batch_{batch_num:02d}.json"
                batch_file.parent.mkdir(parents=True, exist_ok=True)
                with open(batch_file, 'w', encoding='utf-8') as f:
                    json.dump(batch_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ ãƒãƒƒãƒ {batch_num} ã‚’ä¿å­˜: {batch_file}")
        
        # æ¤œè¨¼
        validated_data = self.validate_dataset(all_data)
        
        # æœ€çµ‚å‡ºåŠ›
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.config.output_dir / "processed" / f"gyaru_dataset_{timestamp}.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(validated_data, f, ensure_ascii=False, indent=2)
        
        self.console.print(f"\n[bold green]ğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆå®Œäº†ï¼[/bold green]")
        self.console.print(f"[green]ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}[/green]")
        self.console.print(f"[green]ğŸ“Š ç·ãƒšã‚¢æ•°: {len(validated_data)}[/green]")
        
        # çµ±è¨ˆæƒ…å ±
        topics = {}
        for item in validated_data:
            topic = item.get("topic", "Unknown")
            topics[topic] = topics.get(topic, 0) + 1
        
        self.console.print("\n[cyan]ğŸ“ˆ ãƒˆãƒ”ãƒƒã‚¯åˆ¥çµ±è¨ˆ:[/cyan]")
        for topic, count in sorted(topics.items(), key=lambda x: x[1], reverse=True):
            self.console.print(f"  {topic}: {count}ãƒšã‚¢")
        
        return output_file


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Gyaru Dataset Generator (vLLM APIç‰ˆ)")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/generation_config.yaml"),
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--api-base",
        type=str,
        default=None,
        help="vLLM APIãƒ™ãƒ¼ã‚¹URL (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿)"
    )
    parser.add_argument(
        "--batches",
        type=int,
        default=None,
        help="ç”Ÿæˆãƒãƒƒãƒæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿)"
    )
    
    args = parser.parse_args()
    
    config = DatasetConfig(config_path=args.config)
    if args.api_base:
        config.api_base = args.api_base
    if args.batches:
        config.total_batches = args.batches
    
    generator = GyaruDatasetGenerator(config)
    
    try:
        output_file = generator.generate_dataset()
        console.print(f"\n[bold green]âœ… æˆåŠŸï¼ {output_file} ã‚’ç¢ºèªã—ã¦ãã ã•ã„[/bold green]")
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}", exc_info=True)
        console.print(f"[bold red]âŒ ã‚¨ãƒ©ãƒ¼: {e}[/bold red]")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
