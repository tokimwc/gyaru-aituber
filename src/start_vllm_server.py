"""
vLLM Server Startup Script for Gyaru Dataset Generator

GPTQ-Int8ãƒ¢ãƒ‡ãƒ«ã‚’vLLMã§èµ·å‹•ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
vLLMã®æ¨™æº–OpenAIäº’æ›APIã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨
"""

import argparse
import logging
import sys
import subprocess
from pathlib import Path
import yaml

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_config(config_path: Path = Path("config/generation_config.yaml")):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    if not config_path.exists():
        raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    return config


def start_vllm_server(config_path: Path = None):
    """vLLMã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ï¼ˆvLLMã®æ¨™æº–APIã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ï¼‰"""
    if config_path is None:
        config_path = Path("config/generation_config.yaml")
    
    config = load_config(config_path)
    model_config = config['model']
    vllm_config = config['vllm']
    
    logger.info("=" * 60)
    logger.info("ğŸš€ vLLM Server Starting...")
    logger.info("=" * 60)
    logger.info(f"ğŸ“¦ Model: {model_config['path']}")
    logger.info(f"ğŸ”§ Quantization: {vllm_config['quantization']}")
    logger.info(f"ğŸŒ Host: {vllm_config['host']}:{vllm_config['port']}")
    logger.info("=" * 60)
    
    # vLLMã®æ¨™æº–OpenAIäº’æ›APIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
    # uv run python -m vllm.entrypoints.openai.api_server ã‚’ä½¿ç”¨
    import sys
    import shutil
    
    # uv runã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ï¼ˆvLLMãŒuvç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    uv_path = shutil.which("uv")
    if uv_path:
        cmd = [
            "uv", "run", "--with", "vllm", "--with", "bitsandbytes", "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_config['path'],
            "--host", vllm_config['host'],
            "--port", str(vllm_config['port']),
            "--tensor-parallel-size", str(vllm_config['tensor_parallel_size']),
            "--gpu-memory-utilization", str(vllm_config['gpu_memory_utilization']),
            "--max-model-len", str(vllm_config['max_model_len']),
            "--quantization", vllm_config['quantization'],
            "--dtype", "float16",  # GPTQã¯float16ã®ã¿ã‚µãƒãƒ¼ãƒˆ
        ]
    else:
        # uvãŒãªã„å ´åˆã¯é€šå¸¸ã®pythonã‚’ä½¿ç”¨
        cmd = [
            sys.executable, "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_config['path'],
            "--host", vllm_config['host'],
            "--port", str(vllm_config['port']),
            "--tensor-parallel-size", str(vllm_config['tensor_parallel_size']),
            "--gpu-memory-utilization", str(vllm_config['gpu_memory_utilization']),
            "--max-model-len", str(vllm_config['max_model_len']),
            "--quantization", vllm_config['quantization'],
            "--dtype", "float16",  # GPTQã¯float16ã®ã¿ã‚µãƒãƒ¼ãƒˆ
        ]
    
    if vllm_config['trust_remote_code']:
        cmd.append("--trust-remote-code")
    
    logger.info(f"â³ Starting vLLM server...")
    logger.info(f"Command: {' '.join(cmd)}")
    
    # ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
    try:
        subprocess.run(cmd, check=True)
    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ Server stopped by user")
        sys.exit(0)
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ Server error: {e}")
        sys.exit(1)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="vLLM Server for Gyaru Dataset Generator")
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("config/generation_config.yaml"),
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )
    
    args = parser.parse_args()
    
    try:
        start_vllm_server(args.config)
    except Exception as e:
        logger.error(f"âŒ Error: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
